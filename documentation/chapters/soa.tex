\section{State of the art}
As mentioned previously, choosing C as the sole development language is a deliberate design decision, since it provides all possible concurrency primitives natively in Unix systems (threads, message queues, processes, file locks, etc.), while only requiring the standard library. This fact provides as much freedom as possible to experiment and explore diverse concurrent back-end architectures, while also making the code easily portable to all Unix systems.

Arguably, Go is very well suited to be used as a state of the art comparison to this paper's proposed implementation in C. First of all, Go was created at Google in 2010 by some of the computer science pioneers that originally came up with Unix and C at Bell Labs, so it is no surprise that Go has been described as a "C-like language" or as "C for the 21st Century" \cite{GoPL2015}. Furthermore, it was created with "built-in concurrency" to tackle modern large distributed infrastructure problems and it is currently widely used at all network traffic levels as a server side service provider \cite{Pike2012}. Therefore, it is a great candidate as a point of reference of how modern server side network concurrency is handled, from which a totally different architecture based on blocking processes can be developed. 

If the main goal of this paper is to open a developer's eyes to the many different concurrency paradigms that can be used for server side development, then the philosophy of Go (and for that matter, also of other popular frameworks like Node.js) is the antithesis of this work, because these frameworks provide an inflexible architecture that handles concurrency. In the case of Go, the syntax to handle the creation of concurrent workloads (so-called "\textit{goroutines}") and of communication channels between the goroutines is so simple that an unaware or beginner programmer might be completely oblivious of the scheduling work being performed under the hood by the Go runtime, or even of the fact that its code is running concurrently. 

\subsection{Goroutines}
The idiomatic way of dealing with client connections in Go, either in an HTTP server or through solely raw TCP communication, is by spawning a new goroutine that handles each client concurrently \cite{Morsing2013_2}\cite{GoNet}\cite{GoHTTP}. From a software engineering perspective this is a very practical approach. First, it moves the level of abstraction that the programmer has to deal with to a higher level, where it is unnecessary to directly intervene in memory synchronization and the management of a thread pool. This should have as a consequence gains in developer productivity and a faster development pace through abstraction, with the trade-off that there is less design freedom. The pledge of Go is that the runtime will solely handle the scheduling of goroutines, and that they are so lightweight that the developer should not worry upfront about the amount of goroutines that would simultaneously be spawned \cite{Cox-Buday2017}.

Goroutines are very lightweight concurrent subroutines  supervised by the Go \textit{runtime} in userspace. Their memory footprint is very small, the assigned memory by default is only a few kilobytes at their creation \cite{Cox-Buday2017}. From the perspective of the kernel goroutines are non-preemptive, i.e. they are not interrupted by the OS scheduler to run other goroutines. They have defined \textit{points of entry} where they can be suspended or activated by the runtime scheduler, which is entirely running in userspace. Since a context-switch between goroutines happens in userspace and the runtime decides which data should be persistent between goroutines, it is orders of magnitude faster than context-switching between OS threads \cite{Cox-Buday2017} or between OS processes \cite{Kerrisk2010}. A context-switch between OS threads or processes is a costly operation in terms of both the kernel-side data structures to maintain all threads and processes and the operations performed in kernel space to perform the transition.

\subsection{Runtime scheduler}
Each Go executable is compiled with its own statically linked runtime environment responsible for the scheduling of the goroutines, garbage collection and other tasks. The system model that describes the runtime scheduler consists of three main elements: all statically and dynamically called goroutines, a context, which is the abstraction that actually manages the scheduling of the goroutine queues and the OS threads where the goroutines are run. Each context can be seen as an independent processor with an exclusive runtime environment. Each one has a local queue where goroutines wait to be run in the OS thread currently being used by the context.

Parallelism in the system is achieved by having multiple contexts, each using a different core of the processor through different threads to run the  goroutines waiting in their queues. The runtime manages a set up working threads coupled with contexts, and another set of idle threads waiting to we activated when another thread 

As long as the goroutines running in the contexts do not call a blocking system call, the different goroutines in the queues can be freely interchanged at the given \textit{points of entry} by the scheduler within the same set of OS threads. This, as previously stated, avoids a costly context-switch in kernel space. Nonetheless, if a goroutine performs a syscall that would block, e.g. listens for clients on a TCP socket, the overlying  OS thread in which the context is executing the goroutine would also have to block. 

-----------------------------------

If, another main discussion in the methodology would be the async own database implementation, it would be interesting to talk about locks in any db system, and use the data book as a major reference.

Might as well read and describe how nginx works ?????
